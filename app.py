import streamlit as st
from groq import Groq
from duckduckgo_search import DDGS

# --- 1. åŸºç¤è¨­å®š ---
st.set_page_config(page_title="Llama 3 è¶…ç´šåŠ©æ‰‹", page_icon="âš¡", layout="wide")

# æª¢æŸ¥é‡‘é‘°
if "GROQ_API_KEY" not in st.secrets:
    st.error("è«‹åœ¨ Secrets ä¸­è¨­å®š GROQ_API_KEY")
    st.stop()

# åˆå§‹åŒ– Groq å®¢æˆ¶ç«¯
client = Groq(api_key=st.secrets["GROQ_API_KEY"])

# --- 2. å®šç¾©å¼·å¤§çš„æœå°‹åŠŸèƒ½ (å…è²»ä¸”å³æ™‚) ---
def search_web(query):
    try:
        results = DDGS().text(query, max_results=3)
        if results:
            summary = "\n".join([f"- {r['title']}: {r['body']} (ä¾†æº: {r['href']})" for r in results])
            return summary
        return "ç„¡æœå°‹çµæœã€‚"
    except Exception as e:
        return f"æœå°‹éŒ¯èª¤: {e}"

# --- 3. åˆå§‹åŒ–è¨˜æ†¶é«” ---
if "messages" not in st.session_state:
    # é€™è£¡è¨­å®šã€Œç¬¬ä¸€å‰‡æŒ‡ä»¤ã€ï¼Œç„¡è«–å°è©±å¤šé•·ï¼ŒAI æ°¸é æœƒè¨˜å¾—é€™æ¢
    st.session_state.messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€å€‹æ·±åº¦æ€è€ƒçš„ AI åŠ©æ‰‹ã€‚ä½ å¿…é ˆå…·å‚™ä»¥ä¸‹ç‰¹è³ªï¼š\n1. æ¯æ¬¡å›ç­”å‰ï¼Œå¿…é ˆå…ˆåˆ†æä½¿ç”¨è€…çš„å•é¡Œæ˜¯å¦éœ€è¦ç¶²è·¯è³‡è¨Šã€‚\n2. ä½ çš„å›ç­”å¿…é ˆåŸºæ–¼äº‹å¯¦ã€‚\n3. ç„¡è«–å°è©±é€²è¡Œå¤šä¹…ï¼Œä½ éƒ½å¿…é ˆåš´æ ¼éµå®ˆä½¿ç”¨è€…çš„ç¬¬ä¸€æ¢æŒ‡ä»¤è¨­å®šã€‚"}
    ]

# --- 4. ä»‹é¢è¨­è¨ˆ ---
st.title("Llama 3.3 x å³æ™‚æœå°‹ âš¡")
st.caption("ğŸš€ æ¨¡å‹ï¼šMeta Llama-3.3-70B | æœå°‹ï¼šDuckDuckGo")

# é¡¯ç¤ºæ­·å²å°è©± (ä¸é¡¯ç¤ºç³»çµ±æŒ‡ä»¤ï¼Œåªé¡¯ç¤ºå°è©±)
for msg in st.session_state.messages:
    if msg["role"] != "system":
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

# --- 5. è™•ç†è¼¸å…¥èˆ‡æ€è€ƒ ---
if prompt := st.chat_input("è«‹è¼¸å…¥å•é¡Œ..."):
    # 1. é¡¯ç¤ºä½¿ç”¨è€…å•é¡Œ
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # 2. AI æ€è€ƒèˆ‡æœå°‹
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        
        # æ­¥é©Ÿ A: æœå°‹ç¶²è·¯è³‡æ–™
        with st.status("ğŸ” æ­£åœ¨æœå°‹ç¶²è·¯ä¸¦æ·±åº¦åˆ†æ...", expanded=False) as status:
            search_results = search_web(prompt)
            status.write(f"å·²ç²å–ç¶²è·¯è³‡æ–™ï¼š\n{search_results}")
            status.update(label="âœ… åˆ†æå®Œæˆ", state="complete")
        
        # æ­¥é©Ÿ B: çµ„åˆæœ€çµ‚æç¤ºè© (åŒ…å«æ­·å²ç´€éŒ„ + æœå°‹çµæœ)
        # ç‚ºäº†ç¢ºä¿è¨˜æ†¶åŠ›ï¼Œæˆ‘å€‘å°‡æœå°‹çµæœä½œç‚ºç•¶å‰çš„èƒŒæ™¯çŸ¥è­˜å‚³å…¥
        full_context_prompt = f"ä½¿ç”¨è€…å•é¡Œï¼š{prompt}\n\nåƒè€ƒçš„ç¶²è·¯å³æ™‚è³‡è¨Šï¼š\n{search_results}\n\nè«‹æ ¹æ“šä»¥ä¸Šè³‡è¨Šèˆ‡æ­·å²å°è©±é€²è¡Œæ·±åº¦å›ç­”ï¼š"
        
        # æš«æ™‚æ›¿æ›æœ€å¾Œä¸€æ¢è¨Šæ¯å…§å®¹çµ¦ AI çœ‹ (åŒ…å«æœå°‹çµæœ)ï¼Œä½†ä¸å­˜å…¥æ­·å²ä»¥å…æ··äº‚
        messages_for_ai = st.session_state.messages[:-1] + [{"role": "user", "content": full_context_prompt}]

        try:
            # æ­¥é©Ÿ C: å‘¼å« Llama æ¨¡å‹
            completion = client.chat.completions.create(
                model="llama-3.3-70b-versatile",
                messages=messages_for_ai,
                temperature=0.7,
                max_tokens=4096,
                stream=True,
            )
            
            # ä¸²æµè¼¸å‡ºå›ç­”
            full_response = ""
            for chunk in completion:
                if chunk.choices[0].delta.content:
                    full_response += chunk.choices[0].delta.content
                    message_placeholder.markdown(full_response + "â–Œ")
            
            message_placeholder.markdown(full_response)
            
            # 3. å„²å­˜ AI å›ç­”
            st.session_state.messages.append({"role": "assistant", "content": full_response})

        except Exception as e:
            st.error(f"ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
